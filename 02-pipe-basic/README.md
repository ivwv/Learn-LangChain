# üìò Lesson 02 ‚Äî Building a Prompt ‚Üí Model Pipe Chain

This lesson teaches how to combine a **Prompt Template** and a **Model** into a single reusable pipeline using `.pipe()`.  
This helps us build clean, modular, and scalable AI flows ‚Äî without manually formatting and invoking the model every time.

---

# üöÄ What We Are Doing in This Lesson (Flow Overview)

In this chapter, we build a pipeline that works like this:

1Ô∏è‚É£ **Load environment variables**  
‚Üí So our API keys become available.

2Ô∏è‚É£ **Initialize the Gemini 2.0 Flash model**  
‚Üí This is the LLM that gives final answers.

3Ô∏è‚É£ **Create a prompt template**  
‚Üí A structure like:  
   `"explain me {topic} , like ELI5"`

4Ô∏è‚É£ **Pipe the prompt into the model**  
‚Üí This automatically forms:  
   `formatted prompt ‚Üí model ‚Üí response`

5Ô∏è‚É£ **Call the chain with an input (`{topic: "ice cream"}`)**  
‚Üí LangChain internally formats the prompt and sends it to the LLM.

6Ô∏è‚É£ **Print raw & clean content from the response**  
‚Üí Understand what the model returns.

This single chain forms the foundation of more advanced pipelines like RAG, tools, and agents.

---

# üî• Full Flow Diagram

```
Input (topic: "ice cream")
        ‚îÇ
        ‚ñº
PromptTemplate --- fills {topic} ---> "explain ice cream, like ELI5"
        ‚îÇ
        ‚ñº
Gemini 2.0 Flash LLM
        ‚îÇ
        ‚ñº
Final AI Response
```

Everything between input ‚Üí final response is handled automatically by `.pipe()`.

---

# üß† Code Explained in Logical Blocks

---

## üîπ **1. Setup: Load environment + import LangChain**
```js
import {config} from "dotenv"
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { PromptTemplate } from "@langchain/core/prompts";

config()
```

### ‚úî What this block does:
- Loads `.env` file
- Makes your API key available (`process.env.GEMINI_API_KEY`)
- Imports the LangChain model and prompt classes

Without this setup, nothing else works.

---

## üîπ **2. Create the Gemini Model**
```js
const model = new ChatGoogleGenerativeAI({
    model:"gemini-2.0-flash",
    apiKey:process.env.GEMINI_API_KEY
})
```

### ‚úî Why this block exists:
- Initializes Google Gemini 2.0 Flash model  
- This model processes the final prompt  
- It‚Äôs fast, cheap, and great for chain testing

This is your **AI brain**.

---

## üîπ **3. Create a Prompt Template**
```js
const prompt = PromptTemplate.fromTemplate(`
    explain me {topic} , like ELI5`
)
```

### ‚úî Why this block exists:
- `{topic}` is a dynamic placeholder  
- We can reuse this prompt for **any input topic**
- No need to manually write strings for every call

It makes your prompts **clean, reusable, maintainable**.

---

## üîπ **4. Create a Pipe Chain (Prompt ‚Üí Model)**
```js
const chain = prompt.pipe(model)
```

### ‚úî What this block does:
`.pipe()` connects the prompt template to the model:

```
Input ‚Üí PromptTemplate.format() ‚Üí Model.invoke() ‚Üí Response
```

### ‚úî Why this is powerful:
- You don't need to call `.format()` manually  
- No need to invoke the model manually  
- LangChain handles everything internally  
- Your chain becomes a single clean function

This is how real AI pipelines are built.

---

## üîπ **5. Execute the chain**
```js
const res = await chain.invoke({topic:"ice cream"})
```

### ‚úî Why this block exists:
- You only pass **one object** to the entire pipeline
- LangChain automatically:
  1. Replaces `{topic}`  
  2. Creates the final prompt  
  3. Sends to Gemini  
  4. Returns structured output

Simplest possible pipeline execution.

---

## üîπ **6. Print raw & cleaned output**
```js
console.log("raw response", res)
console.log("chain content response", res.content)
```

### ‚úî Why this block is important:
- `raw response` ‚Üí shows full metadata  
- `res.content` ‚Üí clean text from the LLM

Understanding both is essential when building tools, agents, or RAG systems later.

---

# üîÅ Full Code (Reference)

```js
import {config} from "dotenv"
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { PromptTemplate } from "@langchain/core/prompts";

config()
const model = new ChatGoogleGenerativeAI({
    model:"gemini-2.0-flash",
    apiKey:process.env.GEMINI_API_KEY
})

const prompt = PromptTemplate.fromTemplate(`
    explain me {topic} , like ELI5`
)

// Create chain: prompt -> model

const chain = prompt.pipe(model)
// chain = (input) => model.invoke( prompt.format(input) )

async function run(){
    const res = await chain.invoke({topic:"ice cream"})
    console.log("raw response", res)
    console.log("chain content response", res.content)
}
run().catch(console.error)
```

---

# ‚ñ∂Ô∏è How to Run

```
node 02-pipe-basic.js
```

---

# üåç Real-World Use Cases

- Reusable AI teaching template  
- Chatbots with dynamic prompts  
- Customer support FAQ explainers  
- Educational apps  
- AI writing assistants  
- Multi-step LLM workflows  
- Pipelines combining prompt ‚Üí model ‚Üí output parser  

`.pipe()` is used EVERYWHERE in advanced Agentic AI.

---

# ‚≠ê Next Chapter  
Continue to **Lesson 03 ‚Äî Output Parsers**.

