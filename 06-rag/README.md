# ğŸ“˜ Lesson 06 â€” RAG (Retrieval-Augmented Generation) With Block-by-Block Explanation

This lesson teaches **exactly how a basic RAG pipeline works**, explained in the *same order as your code is written*.  
Each block of code has a matching explanation right under it.

This is the cleanest, easiest way to learn RAG.

---

# ğŸ§  Before the Code: What Is RAG?

RAG = **Retrieval Augmented Generation**

It means:

1ï¸âƒ£ AI retrieves relevant information from your documents  
2ï¸âƒ£ AI injects that info into the prompt  
3ï¸âƒ£ AI answers using ONLY that info  

So the model stops hallucinating and starts answering based on facts.

RAG = *AI that reads your data before answering*.

Used in:

- Perplexity  
- ChatGPT Retrieval  
- AI chatbots with memory  
- Enterprise search  
- Document Q&A  
- Multi-agent knowledge systems  

---

# ğŸ”¥ Full Pipeline Diagram (Matches Code Order)

```
Load API Keys
      â†“
Initialize LLM
      â†“
Initialize Embedding Model
      â†“
Create Vector Store
      â†“
Seed Documents
      â†“
User Asks a Question
      â†“
Similarity Search (find best documents)
      â†“
Merge docs â†’ context
      â†“
Build RAG Prompt (context + question)
      â†“
LLM Generates Final Answer
```

---

# ğŸ§© **CODE EXPLAINED BLOCK BY BLOCK (IN YOUR SEQUENCE)**

---

## ğŸ”¹ **BLOCK 1 â€” Imports + dotenv Setup**
```js
import { config } from "dotenv";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { GoogleGenerativeAIEmbeddings } from "@langchain/google-genai";
import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory";
import { PromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";

config();
```

### âœ” What this block does:
- Loads `.env` so API keys work  
- Imports:
  - The LLM  
  - The embedding model  
  - The vector store  
  - Prompt template  
  - Output parser  

**This is the setup required for any RAG pipeline.**

---

## ğŸ”¹ **BLOCK 2 â€” Create the LLM (Gemini 2.0 Flash)**

```js
const model = new ChatGoogleGenerativeAI({
  model: "gemini-2.0-flash",
  apiKey: process.env.GEMINI_API_KEY,
});
```

### âœ” Explanation:
This is the **AI brain** that will generate the final answer.  
It does NOT know your documents unless you pass context into its prompt.

---

## ğŸ”¹ **BLOCK 3 â€” Create the Embedding Model**

```js
const embeddings = new GoogleGenerativeAIEmbeddings({
  model: "text-embedding-004",
  apiKey: process.env.GEMINI_API_KEY,
});
```

### âœ” Explanation:
Embeddings convert text â†’ vectors (arrays of numbers).  
This allows **semantic search** (search by meaning).

Example:  
â€œWho wants 15 LPA?â€ matches â€œParesh is aiming for 15 LPA.â€

---

## ğŸ”¹ **BLOCK 4 â€” Create the Vector Store (In-Memory DB)**

```js
const vectorStore = new MemoryVectorStore(embeddings);
```

### âœ” Explanation:
- Stores text embeddings  
- Lets you retrieve similar documents  
- Works like Pinecone, but 100% local  
- Fast and perfect for learning RAG  

This acts as your **AIâ€™s long-term memory**.

---

## ğŸ”¹ **BLOCK 5 â€” Seed the Memory With Documents**

```js
await vectorStore.addDocuments([
  { pageContent: "Paresh is building an Agentic AI Backend OS using LangChain, Puppeteer, and Pinecone." },
  { pageContent: "Paresh is aiming for a 15 LPA package by mastering MERN, AI, agents, and RAG." },
  { pageContent: "LangChain Runnables and Tools help create Perplexity-style AI systems." },
]);
```

### âœ” Explanation:
These documents become the **knowledge base** of your RAG system.

Every document is embedded and stored in vector memory.

Now the AI can â€œrememberâ€ these facts.

---

## ğŸ”¹ **BLOCK 6 â€” User Asks a Question**

```js
const question = "Who is trying to reach 15 LPA and what is he building? why ?";
```

### âœ” Explanation:
The user query that requires understanding + factual retrieval.

---

## ğŸ”¹ **BLOCK 7 â€” Retrieve Similar Documents**

```js
const similarDocs = await vectorStore.similaritySearch(question, 3);
```

### âœ” Explanation:
This step:

- Embeds the question  
- Compares it with all stored document vectors  
- Returns the **top 3 semantically similar documents**

This is the â€œRetrievalâ€ part of RAG.

---

## ğŸ”¹ **BLOCK 8 â€” Merge Retrieved Docs into Context**

```js
const context = similarDocs.map(d => d.pageContent).join("\n");
```

### âœ” Explanation:
We convert all retrieved documents into one big CONTEXT block.

LLMs cannot read databases â†’ we must inject the context into the prompt.

---

## ğŸ”¹ **BLOCK 9 â€” Build the RAG Prompt**

```js
const prompt = PromptTemplate.fromTemplate(`
Use the context to answer.

CONTEXT:
{context}

QUESTION:
{question}

ANSWER:
`);
```

### âœ” Explanation:
This template forces the AI to:

- Use provided context  
- Not hallucinate  
- Answer clearly  
- Stay grounded in facts  

This completes the â€œAugmentedâ€ part of RAG.

---

## ğŸ”¹ **BLOCK 10 â€” Build the Chain (Prompt â†’ Model â†’ Parser)**

```js
const chain = prompt.pipe(model).pipe(new StringOutputParser());
```

### âœ” Explanation:
This converts the whole RAG pipeline into a simple chain:

```
input â†’ fill prompt â†’ run LLM â†’ parse string â†’ final answer
```

---

## ğŸ”¹ **BLOCK 11 â€” Get Final Answer**

```js
const answer = await chain.invoke({ context, question });
```

### âœ” Explanation:
We pass:

- the retrieved context  
- the question  

â†’ AI returns a clean, factual answer.

---

## ğŸ”¹ **BLOCK 12 â€” Print Answer**

```js
console.log(answer);
```

### âœ” Explanation:
This is the final output of your RAG pipeline.

---

# ğŸŒ Real Use Cases

- Perplexity-style search  
- Chatbots that read your content  
- Document Q&A  
- Enterprise knowledge assistants  
- Product search  
- Resume/job matching  
- AI assistants with real memory  

Every real AI app uses some form of RAG.

---

# â­ Next Chapter  
**Lesson 07 â€” tool basic** 

