# ğŸ“˜ Lesson 01 â€” Understanding Prompt Templates (Explained Line by Line)

This lesson teaches the **most basic and most important foundation** of LangChain JS:

âœ” How to load environment variables  
âœ” How to initialize the Gemini LLM  
âœ” How to use PromptTemplate  
âœ” How to fill a template with dynamic inputs  
âœ” How to run the model and read the response  

Every single line of code is explained so you fully understand whatâ€™s happening.

---

# ğŸ¯ Purpose

LLMs require **prompts**, but manually writing strings is messy.  
We solve this by using LangChainâ€™s **PromptTemplate**, which allows:

- Clean formatting  
- Dynamic input (like `{topic}`)  
- Reusability  
- Consistent structure  
- Error-free prompts  

This is the **first building block** of all AI apps.

---

# ğŸ”¥ Line-by-Line Code Explanation

### **1ï¸âƒ£ Load .env variables**
```js
import {config} from 'dotenv';
config();
```

âœ” `dotenv` is used to load secrets (API keys) from `.env`.  
âœ” `config()` tells Node.js to read `.env` and add variables to `process.env`.

Without this, your `GEMINI_API_KEY` wonâ€™t load.

---

### **2ï¸âƒ£ Import Gemini Model**
```js
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
```

âœ” This imports the **ChatGoogleGenerativeAI** class.  
âœ” It allows LangChain to communicate with Googleâ€™s Gemini 2.0 Flash model.

---

### **3ï¸âƒ£ Import PromptTemplate**
```js
import { PromptTemplate } from '@langchain/core/prompts';
```

âœ” This gives you the ability to create dynamic text prompts with placeholders.  
âœ” Like `"Explain {topic}"`.

---

### **4ï¸âƒ£ Initialize the Gemini LLM**
```js
const model = new ChatGoogleGenerativeAI({
    model:"gemini-2.0-flash",
    apiKey:process.env.GEMINI_API_KEY
})
```

Breakdown:

- `model:` â†’ selects the model version.
- `"gemini-2.0-flash"` â†’ Googleâ€™s latest fast model (amazing for chat + reasoning).
- `apiKey:` â†’ loads your API key from `.env`.

This object now represents your AI brain.

---

### **5ï¸âƒ£ Create a Prompt Template**
```js
const prompt = PromptTemplate.fromTemplate(`
    explain me {topic} , like ELI5
    `)
```

Breakdown:

- `PromptTemplate.fromTemplate` creates a structured prompt.
- `{topic}` is a variable placeholder.
- You can reuse this template for any topic.

Example output after filling:
```
explain me ice cream, like ELI5
```

---

### **6ï¸âƒ£ Log the template (optional)**
```js
console.log("prompt without fill", prompt)
```

âœ” This shows the template object structure.  
âœ” Helps you understand what LangChain creates behind the scenes.

---

### **7ï¸âƒ£ Create an async function for execution**
```js
async function run(){
```

âœ” Model calls are async â†’ they return `Promise`s.  
âœ” We wrap the logic inside `run()` so we can `await` everything.

---

### **8ï¸âƒ£ Fill the template**
```js
const filledPrompt = await prompt.format({topic:"ice cream"})
console.log(filledPrompt)
```

Breakdown:

- `prompt.format()` â†’ replaces `{topic}` with `"ice cream"`.
- Now the final ready-to-send text is generated.
- `console.log()` prints:

```
explain me ice cream , like ELI5
```

This is the EXACT prompt sent to the LLM.

---

### **9ï¸âƒ£ Invoke the model**
```js
const res = await model.invoke(filledPrompt)
```

Breakdown:

- `model.invoke()` sends the prompt to Gemini.
- Gemini processes it and returns a structured response object.

Inside the response:
- `res.content` â†’ contains the actual text reply.
- Other metadata like tokens may also be present.

---

### **ğŸ”Ÿ Print the final response**
```js
console.log(res.content)
```

âœ” This prints the AIâ€™s answer.  
âœ” Example:

```
Ice cream is a cold sweet dessert made by freezing milk...
```

---

### **1ï¸âƒ£1ï¸âƒ£ Start the function**
```js
run().catch(console.error)
```

âœ” Runs the `run()` function.  
âœ” Catches errors (for example, missing API key).

---

# ğŸ’¡ Full Code (Refresher)

```js
import {config} from 'dotenv';
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { PromptTemplate } from '@langchain/core/prompts';
config()

const model = new ChatGoogleGenerativeAI({
    model:"gemini-2.0-flash",
    apiKey:process.env.GEMINI_API_KEY
})

const prompt = PromptTemplate.fromTemplate(`
    explain me {topic} , like ELI5
    `)

console.log("prompt without fill", prompt)

async function run(){
    const filledPrompt = await prompt.format({topic:"ice cream"})
    console.log(filledPrompt)

    const res = await model.invoke(filledPrompt)
    console.log(res.content)
}

run().catch(console.error)
```

---

# ğŸ§  Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  .env (API KEY)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ loads
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gemini Model Init  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ uses
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt Template "Explain {topic}" â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ format({topic})
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Filled Prompt         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ invoke()
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gemini Response       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸš€ How to Run

### 1. Install required packages
```
npm install
```

### 2. Create `.env` file in root
```
GEMINI_API_KEY=your_api_key_here
```

### 3. Run the file
```
node 01-prompt-chain.js
```

---

# ğŸŒ Real-World Use Cases

- Automated explanations  
- Educational bots  
- Customer support replies  
- Simple Q&A systems  
- Dynamic content generators  
- Email drafting with variables  
- Multi-step AI workflows  

---

# â­ Next Chapter
Proceed to **02 â€” Basic Pipe Flow**.

